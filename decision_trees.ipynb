{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "## Definition\n",
    "\n",
    "Decision trees classify instances by sorting them down the tree from the root to some leaf node, which provides the classification of the instance. Each node in the tree specifies a test of some attribute of the instance, and each branch descending from that node corresponds to one of the possible values for this attribute. An instance is classified by starting at the root node of the tree, testing the attribute specified by this node, then moving down the tree branch corresponding to the value of the attribute in the given example. This process is then repeated for the subtree rooted at the new node.\n",
    "\n",
    "Tree node represents some attribute. Values of the attributes are branches of the tree. So, mathematically decision tree represents a disjunction of conjunctions of constraints on the attribute values of instances.\n",
    "\n",
    "Decision tree learning is generally best suited to problems with the following characteristics:\n",
    "* Instances are represented by attribute-valuepairs.\n",
    "* The target function has discrete output values.\n",
    "* Disjunctive descriptions may be required.\n",
    "* The training data may contain errors.\n",
    "* The training data may contain missing attribute values.\n",
    "\n",
    "## Algorithms\n",
    "\n",
    "### ID3\n",
    "ID3 (Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan. The algorithm creates a multiway tree, finding for each node (i.e. in a greedy manner) the categorical feature that will yield the largest information gain for categorical targets. Trees are grown to their maximum size and then a pruning step is usually applied to improve the ability of the tree to generalise to unseen data.\n",
    "\n",
    "### C4.5\n",
    "C4.5 is the successor to ID3 and removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules. These accuracy of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a rule’s precondition if the accuracy of the rule improves without it.\n",
    "\n",
    "### C5.0\n",
    "C5.0 is Quinlan’s latest version release under a proprietary license. It uses less memory and builds smaller rulesets than C4.5 while being more accurate.\n",
    "\n",
    "### CART\n",
    "CART (Classification and Regression Trees) is very similar to C4.5, but it differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold that yield the largest information gain at each node.\n",
    "\n",
    "scikit-learn uses an optimised version of the CART algorithm; however, scikit-learn implementation does not support categorical variables for now.\n",
    "\n",
    "## information Gain\n",
    "\n",
    "Information gain is the measure of attribute importance to select this as the current top node of the tree. It's basically entropy of positive and negative examples of the attribute values in the collection $S$.\n",
    "\n",
    "$$H(S) = -p_{\\oplus} log_2 p_{\\oplus} - p_{\\ominus} log_2 p_{\\ominus}$$\n",
    "\n",
    "The formula is suitable for binary classification task. For multicategory classification task on set of categories $C$ the formula should be modified as follows:\n",
    "\n",
    "$$H(S) = -\\sum_{c \\in C} p_c log_2 p_c$$\n",
    "\n",
    "Here $p_c$ is the probability of category $c$ for the collection of examples $S$. The value of $p_c$ is expressed as fraction of number of examples signed by category $c$ of number of all examples in the collection.\n",
    "\n",
    "This is the measure of all train collection's entropy - the base value to calculate **information gain**:\n",
    "\n",
    "$$G(S,A) = H(S) - \\sum_{v \\in A} {\\frac{|S_v|}{|S|} H(S_v)}$$\n",
    "\n",
    "Information gain is used in algorithm ID3 (and its enhancement C4.5) to select the best attribute to build the current node.\n",
    "\n",
    "### Build ID3 Tree\n",
    "\n",
    "1. compute the entropy for data-set\n",
    "2. for every attribute/feature:\n",
    "       1. calculate entropy for all categorical values\n",
    "       2. take average information entropy for the current attribute\n",
    "       3. calculate gain for the current attribute\n",
    "3. pick the highest gain attribute.\n",
    "4. Repeat until we get the tree we desired.\n",
    "\n",
    "## Gini Impurity\n",
    "\n",
    "Gini impurity is used in algorithm Classification and Regression Trees (CART) to select the best attribute to build the current node.\n",
    "\n",
    "$$G(S,A) = 1 - \\sum_{c \\in C} p_c^2$$\n",
    "\n",
    "### Build CART Tree\n",
    "\n",
    "1. compute the gini impurity for data-set\n",
    "2. for every attribute/feature:\n",
    "       1. calculate gini impurity gain for all categorical values\n",
    "       2. take average gini impurity for the current attribute \n",
    "       3. calculate the gini gain\n",
    "3. pick the best gini gain attribute.\n",
    "4. Repeat until we get the tree we desired.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembple Methods\n",
    "## Definition\n",
    "\n",
    "Ensemble methods, which combines several decision trees to produce better predictive performance than utilizing a single decision tree. The main principle behind the ensemble model is that a group of weak learners come together to form a strong learner.\n",
    "\n",
    "Let’s talk about few techniques to perform ensemble decision trees:\n",
    "1. Bagging\n",
    "2. Boosting\n",
    "\n",
    "## Bagging\n",
    "\n",
    "**Bagging** (Bootstrap Aggregation) is used when our goal is to reduce the variance of a decision tree. Here idea is to create several subsets of data from training sample chosen randomly with replacement. Now, each collection of subset data is used to train their decision trees. As a result, we end up with an ensemble of different models. Average of all the predictions from different trees are used which is more robust than a single decision tree.\n",
    "\n",
    "**Random Forest** is an extension over bagging. It takes one extra step where in addition to taking the random subset of data, it also takes the random selection of features rather than using all features to grow trees. When you have many random trees. It’s called Random Forest.\n",
    "\n",
    "### Advantages of using Random Forest technique:\n",
    "\n",
    "* Handles higher dimensionality data very well.\n",
    "* Handles missing values and maintains accuracy for missing data.\n",
    "\n",
    "### Disadvantages of using Random Forest technique:\n",
    "\n",
    "Since final prediction is based on the mean predictions from subset trees, it won’t give precise values for the regression model.\n",
    "\n",
    "## Boosting\n",
    "\n",
    "Boosting is another ensemble technique to create a collection of predictors. In this technique, learners are learned sequentially with early learners fitting simple models to the data and then analyzing data for errors. In other words, we fit consecutive trees (random sample) and at every step, the goal is to solve for net error from the prior tree.\n",
    "\n",
    "**Gradient Boosting** is an extension over boosting method.\n",
    "\n",
    "Gradient Boosting = Gradient Descent + Boosting.\n",
    "\n",
    "It uses gradient descent algorithm which can optimize any differentiable loss function. An ensemble of trees are built one by one and individual trees are summed sequentially. Next tree tries to recover the loss (difference between actual and predicted values).\n",
    "\n",
    "### Advantages of using Gradient Boosting technique:\n",
    "\n",
    "* Supports different loss function.\n",
    "* Works well with interactions.\n",
    "\n",
    "### Disadvantages of using Gradient Boosting technique:\n",
    "\n",
    "* Prone to over-fitting.\n",
    "* Requires careful tuning of different hyper-parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes:  ['setosa' 'versicolor' 'virginica']\n",
      "featurers:  ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "train:  (112, 4)\n",
      "test:  (38, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=3, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=0, splitter='best')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "print('classes: ', iris.target_names)\n",
    "print('featurers: ', iris.feature_names)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "print('train: ', X_train.shape)\n",
    "print('test: ', X_test.shape)\n",
    "\n",
    "estimator = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)\n",
    "estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters of DecisionTreeClassifier\n",
    "\n",
    "## criterion\n",
    "\n",
    "string, optional (default=”gini”)\n",
    "\n",
    "The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.\n",
    "\n",
    "## splitter\n",
    "\n",
    "string, optional (default=”best”)\n",
    "\n",
    "The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.\n",
    "\n",
    "## max_depth\n",
    "\n",
    "int or None, optional (default=None)\n",
    "\n",
    "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "## min_samples_split\n",
    "\n",
    "int, float, optional (default=2)\n",
    "\n",
    "The minimum number of samples required to split an internal node:\n",
    "\n",
    "* If int, then consider min_samples_split as the minimum number.\n",
    "* If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "\n",
    "## min_samples_leaf\n",
    "\n",
    "int, float, optional (default=1)\n",
    "\n",
    "The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n",
    "\n",
    "* If int, then consider min_samples_leaf as the minimum number.\n",
    "* If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
    "\n",
    "## min_weight_fraction_leaf\n",
    "\n",
    "float, optional (default=0.)\n",
    "\n",
    "The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
    "\n",
    "## max_features\n",
    "\n",
    "int, float, string or None, optional (default=None)\n",
    "\n",
    "The number of features to consider when looking for the best split:\n",
    "* If int, then consider max_features features at each split.\n",
    "* If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split.\n",
    "* If “auto”, then max_features=sqrt(n_features).\n",
    "* If “sqrt”, then max_features=sqrt(n_features).\n",
    "* If “log2”, then max_features=log2(n_features).\n",
    "* If None, then max_features=n_features.\n",
    "\n",
    "Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features.\n",
    "\n",
    "## max_leaf_nodes\n",
    "\n",
    "int or None, optional (default=None)\n",
    "\n",
    "Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n",
    "\n",
    "## min_impurity_decrease\n",
    "\n",
    "float, optional (default=0.)\n",
    "\n",
    "A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following:\n",
    "\n",
    "## presort\n",
    "\n",
    "bool, optional (default=False)\n",
    "\n",
    "Whether to presort the data to speed up the finding of best splits in fitting. For the default settings of a decision tree on large datasets, setting this to true may slow down the training process. When using either a smaller dataset or a restricted depth, this may speed up the training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Estimator Internal Tree Structure\n",
    "\n",
    "The decision estimator has an attribute called tree_, which stores the entire tree structure and allows access to low level attributes. The binary tree tree_ is represented as a number of parallel arrays. The i-th element of each  array holds information about the node `i`. Node 0 is the tree's root.\n",
    "\n",
    "**NOTE**:\n",
    "Some of the arrays only apply to either leaves or split nodes, resp. In this case the values of nodes of the other type are arbitrary!\n",
    "\n",
    "Among those arrays, we have:\n",
    "   - left_child, id of the left child of the node\n",
    "   - right_child, id of the right child of the node\n",
    "   - feature, feature used for splitting the node\n",
    "   - threshold, threshold value at the node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"350pt\" height=\"314pt\"\n",
       " viewBox=\"0.00 0.00 350.00 314.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 310)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-310 346,-310 346,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<path fill=\"#8139e5\" fill-opacity=\"0.054902\" stroke=\"#000000\" d=\"M198,-306C198,-306 75,-306 75,-306 69,-306 63,-300 63,-294 63,-294 63,-235 63,-235 63,-229 69,-223 75,-223 75,-223 198,-223 198,-223 204,-223 210,-229 210,-235 210,-235 210,-294 210,-294 210,-300 204,-306 198,-306\"/>\n",
       "<text text-anchor=\"start\" x=\"71\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">petal width (cm) ≤ 0.8</text>\n",
       "<text text-anchor=\"start\" x=\"101\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.665</text>\n",
       "<text text-anchor=\"start\" x=\"91.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 112</text>\n",
       "<text text-anchor=\"start\" x=\"78.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [37, 34, 41]</text>\n",
       "<text text-anchor=\"start\" x=\"88\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<path fill=\"#e58139\" stroke=\"#000000\" d=\"M105,-179.5C105,-179.5 12,-179.5 12,-179.5 6,-179.5 0,-173.5 0,-167.5 0,-167.5 0,-123.5 0,-123.5 0,-117.5 6,-111.5 12,-111.5 12,-111.5 105,-111.5 105,-111.5 111,-111.5 117,-117.5 117,-123.5 117,-123.5 117,-167.5 117,-167.5 117,-173.5 111,-179.5 105,-179.5\"/>\n",
       "<text text-anchor=\"start\" x=\"30.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"17.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 37</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [37, 0, 0]</text>\n",
       "<text text-anchor=\"start\" x=\"15\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = setosa</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M109.2194,-222.8796C101.8677,-211.6636 93.9036,-199.5131 86.5126,-188.2372\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"89.4014,-186.2598 80.9921,-179.8149 83.5469,-190.0972 89.4014,-186.2598\"/>\n",
       "<text text-anchor=\"middle\" x=\"75.8982\" y=\"-200.5905\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<path fill=\"#8139e5\" fill-opacity=\"0.172549\" stroke=\"#000000\" d=\"M283.5,-187C283.5,-187 147.5,-187 147.5,-187 141.5,-187 135.5,-181 135.5,-175 135.5,-175 135.5,-116 135.5,-116 135.5,-110 141.5,-104 147.5,-104 147.5,-104 283.5,-104 283.5,-104 289.5,-104 295.5,-110 295.5,-116 295.5,-116 295.5,-175 295.5,-175 295.5,-181 289.5,-187 283.5,-187\"/>\n",
       "<text text-anchor=\"start\" x=\"143.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">petal length (cm) ≤ 4.95</text>\n",
       "<text text-anchor=\"start\" x=\"180\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.496</text>\n",
       "<text text-anchor=\"start\" x=\"174.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 75</text>\n",
       "<text text-anchor=\"start\" x=\"161\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 34, 41]</text>\n",
       "<text text-anchor=\"start\" x=\"167\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M164.1304,-222.8796C169.93,-214.1434 176.1059,-204.8404 182.0908,-195.8253\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"185.1368,-197.5652 187.7517,-187.2981 179.3049,-193.6935 185.1368,-197.5652\"/>\n",
       "<text text-anchor=\"middle\" x=\"192.7022\" y=\"-208.103\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<path fill=\"#39e581\" fill-opacity=\"0.909804\" stroke=\"#000000\" d=\"M195,-68C195,-68 98,-68 98,-68 92,-68 86,-62 86,-56 86,-56 86,-12 86,-12 86,-6 92,0 98,0 98,0 195,0 195,0 201,0 207,-6 207,-12 207,-12 207,-56 207,-56 207,-62 201,-68 195,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"111\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.153</text>\n",
       "<text text-anchor=\"start\" x=\"105.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 36</text>\n",
       "<text text-anchor=\"start\" x=\"96\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 33, 3]</text>\n",
       "<text text-anchor=\"start\" x=\"94\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M189.8069,-103.9815C184.3469,-95.1585 178.5716,-85.8258 173.0793,-76.9506\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"175.942,-74.9254 167.7035,-68.2637 169.9896,-78.609 175.942,-74.9254\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<path fill=\"#8139e5\" fill-opacity=\"0.972549\" stroke=\"#000000\" d=\"M330,-68C330,-68 237,-68 237,-68 231,-68 225,-62 225,-56 225,-56 225,-12 225,-12 225,-6 231,0 237,0 237,0 330,0 330,0 336,0 342,-6 342,-12 342,-12 342,-56 342,-56 342,-62 336,-68 330,-68\"/>\n",
       "<text text-anchor=\"start\" x=\"251.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.05</text>\n",
       "<text text-anchor=\"start\" x=\"242.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 39</text>\n",
       "<text text-anchor=\"start\" x=\"233\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 1, 38]</text>\n",
       "<text text-anchor=\"start\" x=\"235\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M240.8207,-103.9815C246.2016,-95.1585 251.8932,-85.8258 257.3059,-76.9506\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"260.3851,-78.6236 262.6038,-68.2637 254.4088,-74.9789 260.3851,-78.6236\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7f3d186e1c50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz \n",
    "from sklearn import tree\n",
    "\n",
    "dot_data = tree.export_graphviz(estimator, out_file=None, \n",
    "                      feature_names=iris.feature_names,  \n",
    "                      class_names=iris.target_names,  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The binary tree structure has 5 nodes and has the following tree structure:\n",
      "node=0 test node: go to node 1 if X[:, 3] <= 0.800000011920929 else to node 2.\n",
      "\tnode=1 leaf node.\n",
      "\tnode=2 test node: go to node 3 if X[:, 2] <= 4.950000047683716 else to node 4.\n",
      "\t\tnode=3 leaf node.\n",
      "\t\tnode=4 leaf node.\n",
      "\n",
      "Rules used to predict sample 0: \n",
      "decision id node 0 : (X_test[0, 3] (= 2.4) > 0.800000011920929)\n",
      "decision id node 2 : (X_test[0, 2] (= 5.1) > 4.950000047683716)\n",
      "\n",
      "The following samples [0, 1] share the node [0 2] in the tree\n",
      "It is 40.0 % of all nodes.\n"
     ]
    }
   ],
   "source": [
    "# Using those arrays, we can parse the tree structure:\n",
    "\n",
    "n_nodes = estimator.tree_.node_count\n",
    "children_left = estimator.tree_.children_left\n",
    "children_right = estimator.tree_.children_right\n",
    "feature = estimator.tree_.feature\n",
    "threshold = estimator.tree_.threshold\n",
    "\n",
    "\n",
    "# The tree structure can be traversed to compute various properties such\n",
    "# as the depth of each node and whether or not it is a leaf.\n",
    "node_depth = np.zeros(shape=n_nodes, dtype=np.int64)\n",
    "is_leaves = np.zeros(shape=n_nodes, dtype=bool)\n",
    "stack = [(0, -1)]  # seed is the root node id and its parent depth\n",
    "while len(stack) > 0:\n",
    "    node_id, parent_depth = stack.pop()\n",
    "    node_depth[node_id] = parent_depth + 1\n",
    "\n",
    "    # If we have a test node\n",
    "    if (children_left[node_id] != children_right[node_id]):\n",
    "        stack.append((children_left[node_id], parent_depth + 1))\n",
    "        stack.append((children_right[node_id], parent_depth + 1))\n",
    "    else:\n",
    "        is_leaves[node_id] = True\n",
    "\n",
    "print(\"The binary tree structure has %s nodes and has \"\n",
    "      \"the following tree structure:\"\n",
    "      % n_nodes)\n",
    "for i in range(n_nodes):\n",
    "    if is_leaves[i]:\n",
    "        print(\"%snode=%s leaf node.\" % (node_depth[i] * \"\\t\", i))\n",
    "    else:\n",
    "        print(\"%snode=%s test node: go to node %s if X[:, %s] <= %s else to \"\n",
    "              \"node %s.\"\n",
    "              % (node_depth[i] * \"\\t\",\n",
    "                 i,\n",
    "                 children_left[i],\n",
    "                 feature[i],\n",
    "                 threshold[i],\n",
    "                 children_right[i],\n",
    "                 ))\n",
    "print()\n",
    "\n",
    "# First let's retrieve the decision path of each sample. The decision_path\n",
    "# method allows to retrieve the node indicator functions. A non zero element of\n",
    "# indicator matrix at the position (i, j) indicates that the sample i goes\n",
    "# through the node j.\n",
    "\n",
    "node_indicator = estimator.decision_path(X_test)\n",
    "\n",
    "# Similarly, we can also have the leaves ids reached by each sample.\n",
    "\n",
    "leave_id = estimator.apply(X_test)\n",
    "\n",
    "# Now, it's possible to get the tests that were used to predict a sample or\n",
    "# a group of samples. First, let's make it for the sample.\n",
    "\n",
    "sample_id = 0\n",
    "node_index = node_indicator.indices[node_indicator.indptr[sample_id]:\n",
    "                                    node_indicator.indptr[sample_id + 1]]\n",
    "\n",
    "print('Rules used to predict sample %s: ' % sample_id)\n",
    "for node_id in node_index:\n",
    "    if leave_id[sample_id] == node_id:\n",
    "        continue\n",
    "\n",
    "    if (X_test[sample_id, feature[node_id]] <= threshold[node_id]):\n",
    "        threshold_sign = \"<=\"\n",
    "    else:\n",
    "        threshold_sign = \">\"\n",
    "\n",
    "    print(\"decision id node %s : (X_test[%s, %s] (= %s) %s %s)\"\n",
    "          % (node_id,\n",
    "             sample_id,\n",
    "             feature[node_id],\n",
    "             X_test[sample_id, feature[node_id]],\n",
    "             threshold_sign,\n",
    "             threshold[node_id]))\n",
    "\n",
    "# For a group of samples, we have the following common node.\n",
    "sample_ids = [0, 1]\n",
    "common_nodes = (node_indicator.toarray()[sample_ids].sum(axis=0) ==\n",
    "                len(sample_ids))\n",
    "\n",
    "common_node_id = np.arange(n_nodes)[common_nodes]\n",
    "\n",
    "print(\"\\nThe following samples %s share the node %s in the tree\"\n",
    "      % (sample_ids, common_node_id))\n",
    "print(\"It is %s %% of all nodes.\" % (100 * len(common_node_id) / n_nodes,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
